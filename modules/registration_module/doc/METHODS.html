<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>METHODS</title>
  <style>
html {
line-height: 1.5;
font-family: Georgia, serif;
font-size: 20px;
color: #1a1a1a;
background-color: #fdfdfd;
}
body {
margin: 0 auto;
max-width: 36em;
padding-left: 50px;
padding-right: 50px;
padding-top: 50px;
padding-bottom: 50px;
hyphens: auto;
overflow-wrap: break-word;
text-rendering: optimizeLegibility;
font-kerning: normal;
}
@media (max-width: 600px) {
body {
font-size: 0.9em;
padding: 1em;
}
h1 {
font-size: 1.8em;
}
}
@media print {
body {
background-color: transparent;
color: black;
font-size: 12pt;
}
p, h2, h3 {
orphans: 3;
widows: 3;
}
h2, h3, h4 {
page-break-after: avoid;
}
}
p {
margin: 1em 0;
}
a {
color: #1a1a1a;
}
a:visited {
color: #1a1a1a;
}
img {
max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
}
h5, h6 {
font-size: 1em;
font-style: italic;
}
h6 {
font-weight: normal;
}
ol, ul {
padding-left: 1.7em;
margin-top: 1em;
}
li > ol, li > ul {
margin-top: 0;
}
blockquote {
margin: 1em 0 1em 1.7em;
padding-left: 1em;
border-left: 2px solid #e6e6e6;
color: #606060;
}
code {
font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
font-size: 85%;
margin: 0;
}
pre {
margin: 1em 0;
overflow: auto;
}
pre code {
padding: 0;
overflow: visible;
overflow-wrap: normal;
}
.sourceCode {
background-color: transparent;
overflow: visible;
}
hr {
background-color: #1a1a1a;
border: none;
height: 1px;
margin: 1em 0;
}
table {
margin: 1em 0;
border-collapse: collapse;
width: 100%;
overflow-x: auto;
display: block;
font-variant-numeric: lining-nums tabular-nums;
}
table caption {
margin-bottom: 0.75em;
}
tbody {
margin-top: 0.5em;
border-top: 1px solid #1a1a1a;
border-bottom: 1px solid #1a1a1a;
}
th {
border-top: 1px solid #1a1a1a;
padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
margin-bottom: 4em;
text-align: center;
}
#TOC li {
list-style: none;
}
#TOC ul {
padding-left: 1.3em;
}
#TOC > ul {
padding-left: 0;
}
#TOC a:not(:hover) {
text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="methods">Methods</h1>
<h2 id="registration">Registration</h2>
<p>The following registration methods are implemented:</p>
<ul>
<li><p><strong>StackReg</strong>: Registration using pyStackReg with
translation transformation. pyStackReg is a Python/C++ port of the
ImageJ extension TurboReg/StackReg written by Philippe Thevenaz/EPFL [1]
(<a href="http://bigwww.epfl.ch/thevenaz/turboreg/"><a href="http://bigwww.epfl.ch/thevenaz/turboreg/">http://bigwww.epfl.ch/thevenaz/turboreg/</a></a>).</p></li>
<li><p><strong>Phase correlation</strong>: Registration using the phase
correlation method implemented in OpenCV (function
<code>phaseCorrelate()</code>), which uses the Fourrier shift theorem to
detect translational shift in the frequency domain (see <a href="https://en.wikipedia.org/wiki/Phase_correlation"><a href="https://en.wikipedia.org/wiki/Phase_correlation">https://en.wikipedia.org/wiki/Phase_correlation</a></a>).
This method fast, but tend to fail when there too many non-moving
artefacts are present in the image (e.g. dust).</p></li>
<li><p><strong>Feature matching</strong>: Four varians of the &quot;feature
matching&quot; registration methods are available (ORB, BRISK, AKAZE and
SIFT). In this method, registration is performed in three steps:</p>
<ol type="1">
<li>Feature (keypoints) detection and evaluation of the descriptors
using methods implemented in OpenCV. Four keypoints detector an
descriptor extractor algorithms are available:</li>
</ol>
<ul>
<li><p>ORB (Oriented FAST and Rotated BRIEF) [2].</p></li>
<li><p>BRISK (Binary Robust invariant scalable keypoints) [3].</p></li>
<li><p>AKAZE (Accelerated-KAZE) [4].</p></li>
<li><p>SIFT (scale-invariant feature transform) [5].</p></li>
</ul>
<ol start="2" type="1">
<li><p>Feature matching. Features found in consecutive image frames are
matched using the FLANN-based descriptor matcher implemented in OpenCV
(FLANN stands for Fast Library for Approximate Nearest Neighbors).
Matches are further filtered using the distance ratio test proposed by
Lowe [5] (with threshold 0.75).</p></li>
<li><p>Parameter estimation using RANSAC. The shift between consecutive
image frames is then estimated with the Random sample consensus (RANSAC)
method implemented in scikit-image using a custom transformation model
with translation only.</p></li>
</ol>
<p>Preliminary tests on few sample images suggest that registration
using ORB, BRISK, AKAZE or SIFT algorithms give results of similar
quality. Note that the scale and rotational invariance is not so
important when considering consecutive image frames, as the size and
orientation of the features is not expected to change on short time
scale. However, computation time varies significantly. From fastest to
slowest: ORB, BRISK, AKAZE, SIFT.</p></li>
</ul>
<h2 id="implementation">Implementation</h2>
<ul>
<li>Python (<a href="https://www.python.org/"><a href="https://www.python.org/">https://www.python.org/</a></a>).</li>
<li>OpenCV (<a href="https://opencv.org/"><a href="https://opencv.org/">https://opencv.org/</a></a>).</li>
<li>pyStackReg (<a href="https://github.com/glichtner/pystackreg"><a href="https://github.com/glichtner/pystackreg">https://github.com/glichtner/pystackreg</a></a>).</li>
<li>scikit-image (<a href="https://scikit-image.org/"><a href="https://scikit-image.org/">https://scikit-image.org/</a></a>)</li>
</ul>
<h2 id="references">References</h2>
<p>[1] P. Thevenaz, U. E. Ruttimann and M. Unser (1998). A pyramid
approach to subpixel registration based on intensity. IEEE Transactions
on Image Processing, 7(1), 27–41.</p>
<p>[2] E. Rublee, V. Rabaud, K. Konolige and G. Bradski (2011). ORB: An
efficient alternative to SIFT or SURF. Procedings of the IEEE
International Conference on Computer Vision, 2564–2571.</p>
<p>[3] S. Leutenegger, M. Chli and R. Y. Siegwart (2011). Brisk: Binary
robust invariant scalable keypoints. Procedings of the IEEE
International Conference on Computer Vision, 2548–2555.</p>
<p>[4] P. F. Alcantarilla, J. Nuevo and A. Bartoli (2013). Fast explicit
diffusion for accelerated features in nonlinear scale spaces. Procedings
of the British Machine Vision Conference 2013, 13.1-13.11.</p>
<p>[5] D. G. Lowe (2004). Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of Computer Vision,
60(2), 91–110.</p>
</body>
</html>
